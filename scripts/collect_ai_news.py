#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import json
import requests
import feedparser
import os
from datetime import datetime, timedelta
import re
from urllib.parse import urljoin
import time

class AINewsCollector:
    def __init__(self):
        self.articles = []
        self.sources = [
            {
                'name': 'TechCrunch AI',
                'rss': 'https://techcrunch.com/category/artificial-intelligence/feed/',
                'category': 'ğŸš€ ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—'
            },
            {
                'name': 'MIT Technology Review',
                'rss': 'https://www.technologyreview.com/feed/',
                'category': 'ğŸ“ ç ”ç©¶'
            },
            {
                'name': 'The Verge AI',
                'rss': 'https://www.theverge.com/rss/index.xml',
                'category': 'ğŸ“± ãƒ†ãƒƒã‚¯'
            },
            {
                'name': 'Ars Technica',
                'rss': 'https://feeds.arstechnica.com/arstechnica/index',
                'category': 'âš™ï¸ æŠ€è¡“'
            },
            {
                'name': 'Wired',
                'rss': 'https://www.wired.com/feed/rss',
                'category': 'ğŸ”¬ ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼'
            }
        ]
        
        # AIé–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        self.ai_keywords = [
            'artificial intelligence', 'ai', 'machine learning', 'deep learning',
            'neural network', 'chatgpt', 'gpt', 'openai', 'google ai',
            'microsoft ai', 'claude', 'gemini', 'llm', 'large language model',
            'generative ai', 'automation', 'robot', 'algorithm', 'copilot',
            'bard', 'anthropic', 'midjourney', 'stable diffusion'
        ]

    def clean_text(self, text):
        """HTMLã‚¿ã‚°ã‚’é™¤å»ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ³ã«ã™ã‚‹"""
        if not text:
            return ""
        
        # HTMLã‚¿ã‚°ã‚’é™¤å»
        clean = re.compile('<.*?>')
        text = re.sub(clean, '', text)
        
        # ç‰¹æ®Šæ–‡å­—ã‚’æ­£è¦åŒ–
        text = text.replace('&amp;', '&')
        text = text.replace('&lt;', '<')
        text = text.replace('&gt;', '>')
        text = text.replace('&quot;', '"')
        text = text.replace('&#39;', "'")
        
        return text.strip()

    def is_ai_related(self, title, description):
        """è¨˜äº‹ãŒAIé–¢é€£ã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯"""
        content = (title + ' ' + description).lower()
        return any(keyword in content for keyword in self.ai_keywords)

    def format_date(self, date_string):
        """æ—¥ä»˜ã‚’ç›¸å¯¾è¡¨è¨˜ã«å¤‰æ›"""
        try:
            article_date = datetime.strptime(date_string, '%a, %d %b %Y %H:%M:%S %z')
            now = datetime.now(article_date.tzinfo)
            diff = now - article_date
            
            if diff.days == 0:
                hours = diff.seconds // 3600
                if hours == 0:
                    return "æ•°åˆ†å‰"
                return f"{hours}æ™‚é–“å‰"
            elif diff.days == 1:
                return "1æ—¥å‰"
            elif diff.days < 7:
                return f"{diff.days}æ—¥å‰"
            else:
                return article_date.strftime('%mæœˆ%dæ—¥')
        except:
            return "æœ€è¿‘"

    def fetch_rss(self, url, source_name):
        """RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å–å¾—ã—ã¦è§£æ"""
        try:
            print(f"ğŸ“¡ {source_name} ã‹ã‚‰è¨˜äº‹ã‚’å–å¾—ä¸­...")
            
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’è¨­å®šã—ã¦ã‚¢ã‚¯ã‚»ã‚¹
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            
            # feedparserã§è§£æ
            feed = feedparser.parse(response.content)
            
            if feed.bozo:
                print(f"âš ï¸  {source_name}: ãƒ•ã‚£ãƒ¼ãƒ‰è§£æã«å•é¡ŒãŒã‚ã‚Šã¾ã—ãŸ")
            
            articles = []
            for entry in feed.entries[:15]:  # æœ€æ–°15ä»¶ã‚’ãƒã‚§ãƒƒã‚¯
                try:
                    title = self.clean_text(entry.title)
                    description = self.clean_text(entry.get('description', '') or entry.get('summary', ''))
                    
                    if not title or not entry.link:
                        continue
                    
                    # AIé–¢é€£ã‹ãƒã‚§ãƒƒã‚¯
                    if self.is_ai_related(title, description):
                        article = {
                            'title': title,
                            'summary': description[:300] + ('...' if len(description) > 300 else ''),
                            'url': entry.link,
                            'date': self.format_date(entry.get('published', '')),
                            'source': source_name,
                            'category': '',  # ã‚ã¨ã§è¨­å®š
                            'published_date': entry.get('published', ''),
                            'collected_at': datetime.now().isoformat()
                        }
                        articles.append(article)
                
                except Exception as e:
                    print(f"âš ï¸  è¨˜äº‹å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
            
            print(f"âœ… {source_name}: {len(articles)}ä»¶ã®AIè¨˜äº‹ã‚’å–å¾—")
            return articles
            
        except Exception as e:
            print(f"âŒ {source_name} å–å¾—å¤±æ•—: {e}")
            return []

    def collect_all_news(self):
        """å…¨ã‚½ãƒ¼ã‚¹ã‹ã‚‰è¨˜äº‹ã‚’åé›†"""
        print("ğŸ” AIè¨˜äº‹ã®åé›†ã‚’é–‹å§‹ã—ã¾ã™...")
        
        all_articles = []
        
        for source in self.sources:
            articles = self.fetch_rss(source['rss'], source['name'])
            
            # ã‚«ãƒ†ã‚´ãƒªã‚’è¨­å®š
            for article in articles:
                article['category'] = source['category']
            
            all_articles.extend(articles)
            
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–ã§å°‘ã—å¾…æ©Ÿ
            time.sleep(1)
        
        # é‡è¤‡é™¤å»ï¼ˆURLãƒ™ãƒ¼ã‚¹ï¼‰
        seen_urls = set()
        unique_articles = []
        for article in all_articles:
            if article['url'] not in seen_urls:
                seen_urls.add(article['url'])
                unique_articles.append(article)
        
        # æ—¥ä»˜ã§ã‚½ãƒ¼ãƒˆï¼ˆæ–°ã—ã„é †ï¼‰
        try:
            unique_articles.sort(
                key=lambda x: datetime.fromisoformat(x['collected_at']), 
                reverse=True
            )
        except:
            pass
        
        # æœ€æ–°12ä»¶ã«é™å®š
        self.articles = unique_articles[:12]
        
        print(f"ğŸ‰ åˆè¨ˆ {len(self.articles)} ä»¶ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªAIè¨˜äº‹ã‚’åé›†ã—ã¾ã—ãŸ")
        return self.articles

    def save_to_json(self):
        """è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        # dataãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        os.makedirs('data', exist_ok=True)
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
        data = {
            'last_updated': datetime.now().isoformat(),
            'total_articles': len(self.articles),
            'sources_count': len(self.sources),
            'articles': self.articles
        }
        
        # JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        with open('data/articles.json', 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‚’ data/articles.json ã«ä¿å­˜ã—ã¾ã—ãŸ")

def main():
    collector = AINewsCollector()
    
    try:
        # è¨˜äº‹ã‚’åé›†
        articles = collector.collect_all_news()
        
        if articles:
            # JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            collector.save_to_json()
            print("âœ… AIè¨˜äº‹ã®åé›†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        else:
            print("âš ï¸  è¨˜äº‹ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
            
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

if __name__ == "__main__":
    main()
